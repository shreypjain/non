# NoN Production Environment Configuration
# Copy this file to .env and fill in your values

# =============================================================================
# LLM Provider API Keys (Required - at least one)
# =============================================================================

# Anthropic Claude (Recommended for production - see FINAL_LATENCY_REPORT.md)
ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Google Gemini (Fastest - 25-30x faster than Claude)
GOOGLE_API_KEY=your-google-api-key-here

# OpenAI GPT (Alternative)
OPENAI_API_KEY=your-openai-api-key-here

# =============================================================================
# Scheduler Configuration (Optional)
# =============================================================================

# Maximum concurrent requests per provider
NON_MAX_CONCURRENT_REQUESTS=10

# Requests per minute limit per provider
NON_REQUESTS_PER_MINUTE=60

# Queue strategy: priority, fifo, lifo
NON_QUEUE_STRATEGY=priority

# =============================================================================
# Observability Configuration (Optional)
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
NON_LOG_LEVEL=INFO

# Enable distributed tracing
NON_ENABLE_TRACING=true

# Enable metrics collection
NON_ENABLE_METRICS=true

# Export observability data to database
NON_EXPORT_TO_DATABASE=false

# Database connection string (if NON_EXPORT_TO_DATABASE=true)
NON_DATABASE_URL=postgresql://user:password@localhost:5432/non_observability

# =============================================================================
# Network Configuration (Optional)
# =============================================================================

# Default provider: anthropic, google, openai
NON_DEFAULT_PROVIDER=anthropic

# Default model per provider
NON_ANTHROPIC_MODEL=claude-sonnet-4-5-20250929
NON_GOOGLE_MODEL=gemini-2.0-flash
NON_OPENAI_MODEL=gpt-4o-mini

# Default temperature for LLM calls
NON_DEFAULT_TEMPERATURE=0.7

# Default max tokens per request
NON_DEFAULT_MAX_TOKENS=1024

# =============================================================================
# Error Handling Configuration (Optional)
# =============================================================================

# Error policy: fail_fast, retry_with_backoff, ignore, fallback_to_default
NON_ERROR_POLICY=retry_with_backoff

# Maximum retry attempts
NON_MAX_RETRIES=3

# Retry backoff multiplier (seconds)
NON_RETRY_BACKOFF_MULTIPLIER=2

# Minimum success threshold for parallel layers (0.0 - 1.0)
NON_MIN_SUCCESS_THRESHOLD=0.7

# =============================================================================
# Production Deployment (Optional)
# =============================================================================

# Application environment: development, staging, production
NON_ENVIRONMENT=production

# Enable debug mode (disable in production)
NON_DEBUG=false

# Request timeout (seconds)
NON_REQUEST_TIMEOUT=300

# Enable request caching
NON_ENABLE_CACHE=true

# Cache TTL (seconds)
NON_CACHE_TTL=3600
